{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Regression Comparison\n",
        "\n",
        "This notebook implements and compares different regression algorithms on housing price data:\n",
        "1. Random Forest Regressor\n",
        "2. XGBoost\n",
        "3. CatBoost\n",
        "4. Artificial Neural Network (Keras) ‚Äî with a sklearn MLP fallback if TensorFlow is unavailable\n",
        "5. Linear Regression\n",
        "\n",
        "Notes:\n",
        "- Hyperparameters for the main models are centralized in a single cell so you can tune them easily.\n",
        "- Keras import is attempted but if TensorFlow isn't installed the notebook falls back to sklearn's MLPRegressor to avoid runtime errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries (with safe TensorFlow import)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‚úì All libraries imported successfully (tensorflow available: False) \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import tensorflow / keras; if not available we'll fall back to sklearn MLP for the ANN section\n",
        "TF_AVAILABLE = True\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Dropout\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "except Exception:\n",
        "    TF_AVAILABLE = False\n",
        "    # we'll import sklearn MLPRegressor later if needed\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(f\"‚úì All libraries imported successfully (tensorflow available: {TF_AVAILABLE})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Centralized Hyperparameters\n",
        "\n",
        "Change these values to adjust model complexity / training time in one place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters (change here to tune models)\n",
        "hp = {\n",
        "    # Random Forest\n",
        "    'rf_n_estimators': 300,        # number of trees\n",
        "    'rf_max_depth': 18,\n",
        "    'rf_min_samples_split': 4,\n",
        "    'rf_min_samples_leaf': 2,\n",
        "\n",
        "    # XGBoost\n",
        "    'xgb_n_estimators': 300,\n",
        "    'xgb_max_depth': 5,\n",
        "    'xgb_learning_rate': 0.05,\n",
        "    'xgb_subsample': 0.9,\n",
        "    'xgb_colsample_bytree': 0.8,\n",
        "\n",
        "    # CatBoost\n",
        "    'cb_iterations': 1000,\n",
        "    'cb_learning_rate': 0.03,\n",
        "    'cb_depth': 6,\n",
        "\n",
        "    # ANN (Keras or sklearn fallback)\n",
        "    'ann_epochs': 50,\n",
        "    'ann_batch_size': 64,\n",
        "    'ann_learning_rate': 0.001,\n",
        "    'ann_hidden_units': [128, 64, 32],\n",
        "}\n",
        "\n",
        "hp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "\n",
        "# Separate target variable\n",
        "y = train_df['SalePrice']\n",
        "\n",
        "# Select numerical features excluding 'Id' and 'SalePrice'\n",
        "numerical_features = train_df.select_dtypes(include=np.number).columns.tolist()\n",
        "numerical_features = [feature for feature in numerical_features if feature not in ['Id', 'SalePrice']]\n",
        "\n",
        "# Prepare feature sets\n",
        "X = train_df[numerical_features].copy()\n",
        "X_test_full = test_df[numerical_features].copy()\n",
        "\n",
        "# Handle missing values - fill with mean (use training mean)\n",
        "for col in X.columns:\n",
        "    if X[col].isnull().sum() > 0:\n",
        "        mean_val = X[col].mean()\n",
        "        X[col] = X[col].fillna(mean_val)\n",
        "        X_test_full[col] = X_test_full[col].fillna(mean_val)\n",
        "\n",
        "print(f\"\\nFeatures selected: {len(numerical_features)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Split and Scale Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features \n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Validation set size: {X_val.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Models\n",
        "\n",
        "### 4.1 Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training Random Forest Regressor...\")\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=hp['rf_n_estimators'],\n",
        "    max_depth=hp['rf_max_depth'],\n",
        "    min_samples_split=hp['rf_min_samples_split'],\n",
        "    min_samples_leaf=hp['rf_min_samples_leaf'],\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "rf_pred_train = rf_model.predict(X_train)\n",
        "rf_pred_val = rf_model.predict(X_val)\n",
        "\n",
        "# Calculate metrics\n",
        "rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_pred_train))\n",
        "rf_val_rmse = np.sqrt(mean_squared_error(y_val, rf_pred_val))\n",
        "rf_train_r2 = r2_score(y_train, rf_pred_train)\n",
        "rf_val_r2 = r2_score(y_val, rf_pred_val)\n",
        "\n",
        "print(f\"\\n‚úì Random Forest Results:\")\n",
        "print(f\"  Train RMSE: ${rf_train_rmse:,.2f}\")\n",
        "print(f\"  Val RMSE: ${rf_val_rmse:,.2f}\")\n",
        "print(f\"  Train R¬≤: {rf_train_r2:.4f}\")\n",
        "print(f\"  Val R¬≤: {rf_val_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training XGBoost...\")\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=hp['xgb_n_estimators'],\n",
        "    max_depth=hp['xgb_max_depth'],\n",
        "    learning_rate=hp['xgb_learning_rate'],\n",
        "    subsample=hp['xgb_subsample'],\n",
        "    colsample_bytree=hp['xgb_colsample_bytree'],\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "xgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=False,\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "xgb_pred_train = xgb_model.predict(X_train)\n",
        "xgb_pred_val = xgb_model.predict(X_val)\n",
        "\n",
        "# Calculate metrics\n",
        "xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_pred_train))\n",
        "xgb_val_rmse = np.sqrt(mean_squared_error(y_val, xgb_pred_val))\n",
        "xgb_train_r2 = r2_score(y_train, xgb_pred_train)\n",
        "xgb_val_r2 = r2_score(y_val, xgb_pred_val)\n",
        "\n",
        "print(f\"\\n‚úì XGBoost Results:\")\n",
        "print(f\"  Train RMSE: ${xgb_train_rmse:,.2f}\")\n",
        "print(f\"  Val RMSE: ${xgb_val_rmse:,.2f}\")\n",
        "print(f\"  Train R¬≤: {xgb_train_r2:.4f}\")\n",
        "print(f\"  Val R¬≤: {xgb_val_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training CatBoost...\")\n",
        "\n",
        "cb_model = CatBoostRegressor(\n",
        "    iterations=hp['cb_iterations'],\n",
        "    learning_rate=hp['cb_learning_rate'],\n",
        "    depth=hp['cb_depth'],\n",
        "    eval_metric='RMSE',\n",
        "    random_seed=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "cb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=(X_val, y_val),\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "cb_pred_train = cb_model.predict(X_train)\n",
        "cb_pred_val = cb_model.predict(X_val)\n",
        "\n",
        "# Calculate metrics\n",
        "cb_train_rmse = np.sqrt(mean_squared_error(y_train, cb_pred_train))\n",
        "cb_val_rmse = np.sqrt(mean_squared_error(y_val, cb_pred_val))\n",
        "cb_train_r2 = r2_score(y_train, cb_pred_train)\n",
        "cb_val_r2 = r2_score(y_val, cb_pred_val)\n",
        "\n",
        "print(f\"\\n‚úì CatBoost Results:\")\n",
        "print(f\"  Train RMSE: ${cb_train_rmse:,.2f}\")\n",
        "print(f\"  Val RMSE: ${cb_val_rmse:,.2f}\")\n",
        "print(f\"  Train R¬≤: {cb_train_r2:.4f}\")\n",
        "print(f\"  Val R¬≤: {cb_val_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Artificial Neural Network (Keras with sklearn fallback)\n",
        "\n",
        "If TensorFlow / Keras is available, this cell will use it. Otherwise it will fall back to sklearn's MLPRegressor so the notebook can run without TF installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training ANN (Keras or sklearn MLP fallback)...\")\n",
        "\n",
        "if TF_AVAILABLE:\n",
        "    # Build model\n",
        "    ann_model = Sequential()\n",
        "    ann_model.add(Dense(hp['ann_hidden_units'][0], activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "    ann_model.add(Dropout(0.2))\n",
        "    ann_model.add(Dense(hp['ann_hidden_units'][1], activation='relu'))\n",
        "    ann_model.add(Dropout(0.2))\n",
        "    ann_model.add(Dense(hp['ann_hidden_units'][2], activation='relu'))\n",
        "    ann_model.add(Dense(1))\n",
        "\n",
        "    ann_model.compile(optimizer=Adam(learning_rate=hp['ann_learning_rate']), loss='mean_squared_error')\n",
        "\n",
        "    # Train\n",
        "    history = ann_model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        validation_data=(X_val_scaled, y_val),\n",
        "        epochs=hp['ann_epochs'],\n",
        "        batch_size=hp['ann_batch_size'],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    ann_pred_train = ann_model.predict(X_train_scaled).flatten()\n",
        "    ann_pred_val = ann_model.predict(X_val_scaled).flatten()\n",
        "else:\n",
        "    # sklearn fallback\n",
        "    from sklearn.neural_network import MLPRegressor\n",
        "    print(\"TensorFlow not available ‚Äî using sklearn MLPRegressor as a fallback.\")\n",
        "    mlp_hidden_layer_sizes = tuple(hp['ann_hidden_units'])\n",
        "    ann_model = MLPRegressor(\n",
        "        hidden_layer_sizes=mlp_hidden_layer_sizes,\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        learning_rate_init=hp['ann_learning_rate'],\n",
        "        max_iter=hp['ann_epochs'],\n",
        "        batch_size=hp['ann_batch_size'],\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    ann_model.fit(X_train_scaled, y_train)\n",
        "    ann_pred_train = ann_model.predict(X_train_scaled).flatten()\n",
        "    ann_pred_val = ann_model.predict(X_val_scaled).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "ann_train_rmse = np.sqrt(mean_squared_error(y_train, ann_pred_train))\n",
        "ann_val_rmse = np.sqrt(mean_squared_error(y_val, ann_pred_val))\n",
        "ann_train_r2 = r2_score(y_train, ann_pred_train)\n",
        "ann_val_r2 = r2_score(y_val, ann_pred_val)\n",
        "\n",
        "print(f\"\\n‚úì ANN Results:\")\n",
        "print(f\"  Train RMSE: ${ann_train_rmse:,.2f}\")\n",
        "print(f\"  Val RMSE: ${ann_val_rmse:,.2f}\")\n",
        "print(f\"  Train R¬≤: {ann_train_r2:.4f}\")\n",
        "print(f\"  Val R¬≤: {ann_val_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training Linear Regression...\")\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "lr_pred_train = lr_model.predict(X_train)\n",
        "lr_pred_val = lr_model.predict(X_val)\n",
        "\n",
        "# Calculate metrics\n",
        "lr_train_rmse = np.sqrt(mean_squared_error(y_train, lr_pred_train))\n",
        "lr_val_rmse = np.sqrt(mean_squared_error(y_val, lr_pred_val))\n",
        "lr_train_r2 = r2_score(y_train, lr_pred_train)\n",
        "lr_val_r2 = r2_score(y_val, lr_pred_val)\n",
        "\n",
        "print(f\"\\n‚úì Linear Regression Results:\")\n",
        "print(f\"  Train RMSE: ${lr_train_rmse:,.2f}\")\n",
        "print(f\"  Val RMSE: ${lr_val_rmse:,.2f}\")\n",
        "print(f\"  Train R¬≤: {lr_train_r2:.4f}\")\n",
        "print(f\"  Val R¬≤: {lr_val_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_data = [\n",
        "    {\n",
        "        'Model': 'Random Forest',\n",
        "        'Train RMSE': rf_train_rmse,\n",
        "        'Val RMSE': rf_val_rmse,\n",
        "        'Train MAE': mean_absolute_error(y_train, rf_pred_train),\n",
        "        'Val MAE': mean_absolute_error(y_val, rf_pred_val),\n",
        "        'Train R2': rf_train_r2,\n",
        "        'Val R2': rf_val_r2\n",
        "    },\n",
        "    {\n",
        "        'Model': 'XGBoost',\n",
        "        'Train RMSE': xgb_train_rmse,\n",
        "        'Val RMSE': xgb_val_rmse,\n",
        "        'Train MAE': mean_absolute_error(y_train, xgb_pred_train),\n",
        "        'Val MAE': mean_absolute_error(y_val, xgb_pred_val),\n",
        "        'Train R2': xgb_train_r2,\n",
        "        'Val R2': xgb_val_r2\n",
        "    },\n",
        "    {\n",
        "        'Model': 'CatBoost',\n",
        "        'Train RMSE': cb_train_rmse,\n",
        "        'Val RMSE': cb_val_rmse,\n",
        "        'Train MAE': mean_absolute_error(y_train, cb_pred_train),\n",
        "        'Val MAE': mean_absolute_error(y_val, cb_pred_val),\n",
        "        'Train R2': cb_train_r2,\n",
        "        'Val R2': cb_val_r2\n",
        "    },\n",
        "    {\n",
        "        'Model': 'Keras ANN' if TF_AVAILABLE else 'MLP Regressor',\n",
        "        'Train RMSE': ann_train_rmse,\n",
        "        'Val RMSE': ann_val_rmse,\n",
        "        'Train MAE': mean_absolute_error(y_train, ann_pred_train),\n",
        "        'Val MAE': mean_absolute_error(y_val, ann_pred_val),\n",
        "        'Train R2': ann_train_r2,\n",
        "        'Val R2': ann_val_r2\n",
        "    },\n",
        "    {\n",
        "        'Model': 'Linear Regression',\n",
        "        'Train RMSE': lr_train_rmse,\n",
        "        'Val RMSE': lr_val_rmse,\n",
        "        'Train MAE': mean_absolute_error(y_train, lr_pred_train),\n",
        "        'Val MAE': mean_absolute_error(y_val, lr_pred_val),\n",
        "        'Train R2': lr_train_r2,\n",
        "        'Val R2': lr_val_r2\n",
        "    }\n",
        "]\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "df_comparison = df_comparison.sort_values('Val RMSE')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_idx = df_comparison['Val RMSE'].idxmin()\n",
        "best_model = df_comparison.loc[best_model_idx, 'Model']\n",
        "best_rmse = df_comparison.loc[best_model_idx, 'Val RMSE']\n",
        "best_r2 = df_comparison.loc[best_model_idx, 'Val R2']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"üèÜ BEST MODEL: {best_model}\")\n",
        "print(f\"   Validation RMSE: ${best_rmse:,.2f}\")\n",
        "print(f\"   Validation R¬≤ Score: {best_r2:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot 1: RMSE comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Model', y='Val RMSE', data=df_comparison, palette='viridis')\n",
        "plt.title('Validation RMSE (Lower is Better)')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('RMSE ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print top 10 features for each model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 10 MOST IMPORTANT FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "models_dict = {\n",
        "    'Random Forest': rf_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'CatBoost': cb_model\n",
        "}\n",
        "\n",
        "for model_name, model in models_dict.items():\n",
        "    # Some models might not expose feature_importances_ in the same way; guard access\n",
        "    try:\n",
        "        importance = model.feature_importances_\n",
        "    except Exception:\n",
        "        importance = None\n",
        "\n",
        "    if importance is None:\n",
        "        print(f\"\\n{model_name}: feature importances not available\")\n",
        "        continue\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': numerical_features,\n",
        "        'Importance': importance\n",
        "    }).sort_values('Importance', ascending=False).head(10)\n",
        "    \n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for i, row in importance_df.iterrows():\n",
        "        print(f\"  {row['Feature']:30s}: {row['Importance']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display nice summary table\n",
        "summary_df = df_comparison.copy()\n",
        "summary_df['Val RMSE'] = summary_df['Val RMSE'].apply(lambda x: f\"${x:,.2f}\")\n",
        "summary_df['Train RMSE'] = summary_df['Train RMSE'].apply(lambda x: f\"${x:,.2f}\")\n",
        "summary_df['Val MAE'] = summary_df['Val MAE'].apply(lambda x: f\"${x:,.2f}\")\n",
        "summary_df['Train MAE'] = summary_df['Train MAE'].apply(lambda x: f\"${x:,.2f}\")\n",
        "summary_df['Val R2'] = summary_df['Val R2'].apply(lambda x: f\"{x:.4f}\")\n",
        "summary_df['Train R2'] = summary_df['Train R2'].apply(lambda x: f\"{x:.4f}\")\n",
        "print(summary_df)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
